{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar and homework (10 points total)\n",
    "\n",
    "Today we shall compose encoder-decoder neural networks and apply them to the task of machine translation.\n",
    "\n",
    "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
    "_(img: esciencegroup.files.wordpress.com)_\n",
    "\n",
    "\n",
    "Encoder-decoder architectures are about converting anything to anything, including\n",
    " * Machine translation and spoken dialogue systems\n",
    " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
    " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
    " * Grapheme2phoneme - convert words to transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our task: machine translation\n",
    "\n",
    "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions. This task shows the scale of machine translation while not requiring you to train your model for weeks if you don't use GPU.\n",
    "\n",
    "Before we get to the architecture, there's some preprocessing to be done. ~~Go tokenize~~ Alright, this time we've done preprocessing for you. As usual, the data will be tokenized with WordPunctTokenizer.\n",
    "\n",
    "However, there's one more thing to do. Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
    "\n",
    "One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install subword-nmt &> log\n",
    "!wget https://github.com/yandexdataschool/nlp_course/raw/62c1410427385336ef2bea74b09cba39c956207d/week4_seq2seq/data.txt -O data/data.txt 2> log\n",
    "!wget https://github.com/yandexdataschool/nlp_course/raw/62c1410427385336ef2bea74b09cba39c956207d/week4_seq2seq/utils.py -O utils.py 2> log\n",
    "!wget https://github.com/yandexdataschool/nlp_course/raw/62c1410427385336ef2bea74b09cba39c956207d/week4_seq2seq/dummy_checkpoint.npz -O data/dummy_checkpoint.npz 2> log\n",
    "#thanks to tilda and deephack teams for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "tokenizer = WordPunctTokenizer()\n",
    "def tokenize(x):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# split and tokenize the data\n",
    "with open('data/train.en', 'w') as f_src,  open('data/train.ru', 'w') as f_dst:\n",
    "    for line in open('data/data.txt'):\n",
    "        src_line, dst_line = line.strip().split('\\t')\n",
    "        f_src.write(tokenize(src_line) + '\\n')\n",
    "        f_dst.write(tokenize(dst_line) + '\\n')\n",
    "\n",
    "# build and apply bpe vocs\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    learn_bpe(open('data/train.' + lang), open('data/bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('data/bpe_rules.' + lang))\n",
    "    \n",
    "    with open('data/train.bpe.' + lang, 'w') as f_out:\n",
    "        for line in open('data/train.' + lang):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocabularies\n",
    "\n",
    "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: на территории обустроена бесплатная частная парковка .\n",
      "out: free private parking is available on site .\n",
      "\n",
      "inp: кроме того , в 5 минутах ходьбы работают многочисленные бары и рестораны .\n",
      "out: guests can find many bars and restaurants within a 5 - minute walk .\n",
      "\n",
      "inp: отель san mi@@ gu@@ el расположен в центре мор@@ ели@@ и , в 750 метрах от главной площади города и кафедрального собора .\n",
      "out: hotel san miguel is located in central more@@ lia , 750 metres from the city ’ s main square and cathedral .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_inp = np.array(open('data/train.bpe.ru').read().split('\\n'))\n",
    "data_out = np.array(open('data/train.bpe.en').read().split('\\n'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "[[   0 2688 2943 1108   29    1    1    1]\n",
      " [   0 2922 1834 8035   59 3800   29    1]\n",
      " [   0 6030 2083   29    1    1    1    1]\n",
      " [   0 4927 1870   29    1    1    1    1]\n",
      " [   0 5549 1453   27  592   29    1    1]]\n",
      "\n",
      "back to words\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n"
     ]
    }
   ],
   "source": [
    "# Here's how you cast lines into ids and backwards.\n",
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw source and translation length distributions to estimate the scope of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder model\n",
    "\n",
    "The code below contas a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from utils import infer_length, infer_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel:\n",
    "    def __init__(self, name, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        A simple encoder-decoder model\n",
    "        \"\"\"\n",
    "        self.name, self.inp_voc, self.out_voc = name, inp_voc, out_voc\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "\n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "        \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :returns: initial decoder state tensors, one or many\n",
    "        \"\"\"\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            _, enc_last = tf.nn.dynamic_rnn(\n",
    "                              self.enc0, inp_emb,\n",
    "                              sequence_length=inp_lengths,\n",
    "                              dtype = inp_emb.dtype)\n",
    "        dec_start = self.dec_start(enc_last)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        [prev_dec] = prev_state\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        with tf.variable_scope('dec0'):\n",
    "            new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n",
    "        output_logits = self.logits(new_dec_out)\n",
    "        return [new_dec_state], output_logits\n",
    "\n",
    "    def translate_lines(self, inp_lines, max_len=100):\n",
    "        \"\"\"\n",
    "        Translates a list of lines by greedily selecting most likely next token at each step\n",
    "        :returns: a list of output lines, a sequence of model states at each step\n",
    "        \"\"\"\n",
    "        state = sess.run(self.initial_state, {self.inp: inp_voc.to_matrix(inp_lines)})\n",
    "        outputs = [[self.out_voc.bos_ix] for _ in range(len(inp_lines))]\n",
    "        all_states = [state]\n",
    "        finished = [False] * len(inp_lines)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            state, logits = sess.run([self.next_state, self.next_logits], {**dict(zip(self.prev_state, state)),\n",
    "                                           self.prev_tokens: [out_i[-1] for out_i in outputs]})\n",
    "            next_tokens = np.argmax(logits, axis=-1)\n",
    "            all_states.append(state)\n",
    "            for i in range(len(next_tokens)):\n",
    "                outputs[i].append(next_tokens[i])\n",
    "                finished[i] |= next_tokens[i] == self.out_voc.eos_ix\n",
    "        return out_voc.to_lines(outputs), all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp_voc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9595363ddeab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ^^^ if you get \"variable *** already exists\": re-run this cell again - it will clear all tf operations youve 'built\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_voc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_voc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inp_voc' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "# ^^^ if you get \"variable *** already exists\": re-run this cell again - it will clear all tf operations youve 'built\n",
    "\n",
    "model = BasicModel('model', inp_voc, out_voc)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss (2 points)\n",
    "\n",
    "Our training objetive is almost the same as it was for neural language models:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    :returns: logits of shape [batch, time, voc_size]\n",
    "    \n",
    "    * logits must be a linear output of your neural network.\n",
    "    * logits [:, 0, :] should always predic BOS\n",
    "    * logits [:, -1, :] should be probabilities of last token in out\n",
    "    This function should NOT return logits predicted when taking out[:, -1] as y_prev\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(inp)[0]   \n",
    "    \n",
    "    # Encode inp, get initial state\n",
    "    first_state = model.encode(inp)\n",
    "    \n",
    "    # initial logits: always predict BOS\n",
    "    first_logits = tf.log(tf.one_hot(tf.fill([batch_size], model.out_voc.bos_ix),\n",
    "                                     len(model.out_voc)) + 1e-30)\n",
    "    \n",
    "    # Decode step\n",
    "    def step(prev_state, y_prev):\n",
    "        # Given previous state, obtain next state and next token logits\n",
    "        return [*model.decode(prev_state[0], y_prev)]\n",
    "\n",
    "    # You can now use tf.scan to run step several times.\n",
    "    # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "\n",
    "    states, logits_seq = tf.scan(\n",
    "        step,\n",
    "        elems=tf.transpose(out),\n",
    "        initializer=[first_state, first_logits], \n",
    "    )\n",
    "    \n",
    "    # prepend first_logits to logits_seq\n",
    "    \n",
    "    logits_seq = tf.concat([tf.expand_dims(first_logits, 0), logits_seq[:-1]], axis=0)\n",
    "    # Make sure you convert logits_seq from [time, batch, voc_size] to [batch, time, voc_size]\n",
    "    logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
    "    return logits_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dummy_checkpoint.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-112dbaf911f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/dummy_checkpoint.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdummy_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdummy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdummy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/natural-language-processing/week04_seq2seq/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(variables, path, sess, verbose)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"please make sure you defined a default TF session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mvar_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mvar_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvar_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_values\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mnot_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dummy_checkpoint.npz'"
     ]
    }
   ],
   "source": [
    "from utils import load\n",
    "load(tf.trainable_variables(), 'data/dummy_checkpoint.npz')\n",
    "dummy_inp = tf.constant(inp_voc.to_matrix(train_inp[:3]))\n",
    "dummy_out = tf.constant(out_voc.to_matrix(train_out[:3]))\n",
    "dummy_logits = sess.run(compute_logits(model, dummy_inp, dummy_out))\n",
    "dummy_ref = np.array([-0.13257082, -0.11084784, -0.09024167, -0.14910498], dtype='float32')\n",
    "assert np.allclose(dummy_logits.sum(-1)[0, 1:5], dummy_ref)\n",
    "ref_shape = (dummy_out.shape[0], dummy_out.shape[1], len(out_voc))\n",
    "assert dummy_logits.shape == ref_shape, \"Your logits shape should be {} but got {}\".format(dummy_logits.shape, ref_shape)\n",
    "assert all(dummy_logits[:, 0].argmax(-1) == out_voc.bos_ix), \"first step must always be BOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import select_values_over_last_axis\n",
    "\n",
    "def compute_loss(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    In order to pass the tests, your function should\n",
    "    * include loss at first EOS but not the subsequent ones\n",
    "    * divide sum of losses by a sum of input lengths (use infer_length or infer_mask)\n",
    "    \"\"\"\n",
    "    mask = infer_mask(out, out_voc.eos_ix)    \n",
    "    logits_seq = compute_logits(model, inp, out, **flags)\n",
    "    \n",
    "    # Compute loss as per instructions above\n",
    "    \n",
    "    loss = tf.reduce_sum(\n",
    "        tf.boolean_mask(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=out,\n",
    "                logits=logits_seq, \n",
    "            ),\n",
    "            mask\n",
    "    )) / tf.reduce_sum(mask)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 25 and 3 for 'Attention_2/map/while/add_1' (op: 'Add') with input shapes: [128,25], [128,3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 25 and 3 for 'Attention_2/map/while/add_1' (op: 'Add') with input shapes: [128,25], [128,3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-ffdafe77d4d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdummy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.425\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"We're sorry for your loss\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-517aac1c4517>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, inp, out, **flags)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlogits_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Compute loss as per instructions above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-ffefdbe611dc>\u001b[0m in \u001b[0;36mcompute_logits\u001b[0;34m(model, inp, out, **flags)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Encode inp, get initial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfirst_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# initial logits: always predict BOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-090249986dc1>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, inp, **flags)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# apply attention layer from initial decoder hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mfirst_attn_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Build first state: include\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-ffe327e081d1>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, enc, dec, inp_mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 return tf.tensordot(self.linear_out, self.activ(tf.matmul(self.linear_e, enc, transpose_b=True) + \n\u001b[1;32m     31\u001b[0m                                                     tf.matmul(self.linear_d, dec, transpose_b=True)), axes=1)\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Apply mask - if mask is 0, logits should be -inf or -1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# You may need tf.where\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3272\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3274\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2992\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2993\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2994\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2995\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2927\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2928\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2929\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2930\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2931\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3241\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3242\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3243\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \"\"\"\n\u001b[1;32m    448\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-ffe327e081d1>\u001b[0m in \u001b[0;36mnn\u001b[0;34m(enc)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 return tf.tensordot(self.linear_out, self.activ(tf.matmul(self.linear_e, enc, transpose_b=True) + \n\u001b[0;32m---> 31\u001b[0;31m                                                     tf.matmul(self.linear_d, dec, transpose_b=True)), axes=1)\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Apply mask - if mask is 0, logits should be -inf or -1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 301\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3270\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3272\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3273\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1788\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1789\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1790\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 25 and 3 for 'Attention_2/map/while/add_1' (op: 'Add') with input shapes: [128,25], [128,3]."
     ]
    }
   ],
   "source": [
    "dummy_loss = sess.run(compute_loss(model, dummy_inp, dummy_out))\n",
    "print(\"Loss:\", dummy_loss)\n",
    "assert np.allclose(dummy_loss, 8.425, rtol=0.1, atol=0.1), \"We're sorry for your loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: BLEU\n",
    "\n",
    "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
    "\n",
    "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute.\n",
    "__Note:__ in this assignment we measure token-level bleu with bpe tokens. Most scientific papers report word-level bleu. You can measure it by undoing BPE encoding before computing BLEU. Please stay with the token-level bleu for this assignment, however.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(model, inp_lines, out_lines, **flags):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
    "    translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "    # Note: if you experience out-of-memory error, split input lines into batches and translate separately\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-f378da1eba61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-168-46acc824dc2a>\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(model, inp_lines, out_lines, bpe_sep, **flags)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpe_sep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'@@ '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Note: if you experience out-of-memory error, split input lines into batches and translate separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-7061b50ec3ac>\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, inp_lines, max_len)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             state, logits = sess.run([self.next_state, self.next_logits], {**dict(zip(self.prev_state, state)),\n\u001b[0;32m---> 67\u001b[0;31m                                            self.prev_tokens: [out_i[-1] for out_i in outputs]})\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mall_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder('int32', [None, None])\n",
    "out = tf.placeholder('int32', [None, None])\n",
    "\n",
    "loss = compute_loss(model, inp, out)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in trange(25000):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    feed_dict = {\n",
    "        inp: inp_voc.to_matrix(train_inp[batch_ix]),\n",
    "        out: out_voc.to_matrix(train_out[batch_ix]),\n",
    "    }\n",
    "    \n",
    "    loss_t, _ = sess.run([loss, train_step], feed_dict)\n",
    "    metrics['train_loss'].append((step, loss_t))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 35, \"We kind of need a higher bleu BLEU from you. Kind of right now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Attention Required (4 points)\n",
    "\n",
    "In this section we want you to improve over the basic model by implementing a simple attention mechanism.\n",
    "\n",
    "This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention layer\n",
    "\n",
    "Here you will have to implement a layer that computes a simple additive attention:\n",
    "\n",
    "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
    "\n",
    "* Compute logits with a 2-layer neural network\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Get probabilities from logits, \n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Add up encoder states with probabilities to get __attention response__\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
    "\n",
    "You can learn more about attention layers in the leture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size, activ=tf.tanh,):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size # num units in encoder state\n",
    "        self.dec_size = dec_size # num units in decoder state\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # YOUR CODE - create layer variables\n",
    "            self.linear_e = tf.Variable(initial_value=tf.random_normal([hid_size, enc_size]))\n",
    "            self.linear_d = tf.Variable(initial_value=tf.random_normal([hid_size, dec_size]))\n",
    "            self.linear_out = tf.Variable(initial_value=tf.random_normal([hid_size]))\n",
    "\n",
    "    def __call__(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # Compute logits\n",
    "            def nn(enc):\n",
    "                return tf.tensordot(self.linear_out, self.activ(self.linear_e @ enc + \n",
    "                                                    self.linear_d @ tf.transpose(dec)), axes=1)\n",
    "            logits = tf.map_fn(nn, tf.transpose(enc, [1, 2, 0]))\n",
    "            # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "            # You may need tf.where\n",
    "            with tf.variable_scope('mask'):\n",
    "                logits =  tf.transpose(logits) - (1 - inp_mask)*1e9\n",
    "            \n",
    "            # Compute attention probabilities (softmax)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            # Compute attention response using enc and probs\n",
    "            attn = tf.reduce_sum(tf.expand_dims(probs, 2) * enc, axis=1)\n",
    "            \n",
    "            return attn, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq model with attention\n",
    "\n",
    "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:\n",
    "![img](https://i.imgur.com/6fKHlHb.png)\n",
    "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
    "\n",
    "On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attetion layer.\n",
    "\n",
    "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
    "* Last RNN hidden states (as in basic model)\n",
    "* The whole sequence of encoder outputs (to attend to) and mask\n",
    "* Attention probabilities (to visualize)\n",
    "\n",
    "_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # YOUR CODE - define model layers            \n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            self.attention = AttentionLayer('Attention', hid_size, hid_size, hid_size)\n",
    "            \n",
    "            # END OF YOUR CODE\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        \n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            enc_states, enc_last = tf.nn.dynamic_rnn(\n",
    "                              self.enc0, inp_emb,\n",
    "                              sequence_length=inp_lengths,\n",
    "                              dtype = inp_emb.dtype)\n",
    "        dec_start = self.dec_start(enc_last)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        mask = infer_mask(inp, inp_voc.eos_ix)\n",
    "        _, first_attn_probas = self.attention(enc_states, dec_start, mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        first_state = [dec_start, enc_states, mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order that you've packed in encode\n",
    "        [dec_state, enc_states, mask, prev_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and attn probas given previous decoder state\n",
    "        # * use prev token embedding and attn response to update decoder states (concatenate and feed into decoder cell)\n",
    "        # * predict logits\n",
    "        \n",
    "        next_attn_response, next_attn_probas = self.attention(enc_states, dec_state, mask)\n",
    "\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        with tf.variable_scope('dec0'):\n",
    "            conc = tf.concat([next_attn_response, prev_emb], -1, name='CONCATTT')\n",
    "            new_dec_out, new_dec_state = self.dec0(conc, dec_state)\n",
    "        output_logits = self.logits(new_dec_out)\n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        next_state = [new_dec_state, enc_states, mask, next_attn_probas]\n",
    "        return next_state, output_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# WARNING! this cell will clear your TF graph from the regular model. All trained variables will be gone!\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = AttentiveModel('model_attn', inp_voc, out_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training attentive model\n",
    "\n",
    "We'll reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "inp = tf.placeholder('int32', [None, None])\n",
    "out = tf.placeholder('int32', [None, None])\n",
    "\n",
    "loss = compute_loss(model, inp, out)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': []}\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4732/25000 [4:56:41<21:10:44,  3.76s/it]"
     ]
    }
   ],
   "source": [
    "for _ in trange(25000):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    feed_dict = {\n",
    "        inp: inp_voc.to_matrix(train_inp[batch_ix]),\n",
    "        out: out_voc.to_matrix(train_out[batch_ix]),\n",
    "    }\n",
    "    \n",
    "    loss_t, _ = sess.run([loss, train_step], feed_dict)\n",
    "    metrics['train_loss'].append((step, loss_t))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "\n",
    "# Your model may train slower than the basic one. check that it's at least >30 bleu by 5k steps\n",
    "# Also: you don't have to train for 25k steps. It was chosen by a squirrel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 45, \"Something might be wrong with the model...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as pl\n",
    "import bokeh.models as bm\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "def draw_attention(inp_line, translation, probs):\n",
    "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
    "    inp_tokens = inp_voc.tokenize(inp_line)\n",
    "    trans_tokens = out_voc.tokenize(translation)\n",
    "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
    "    \n",
    "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
    "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
    "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
    "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
    "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
    "    fig.xaxis.major_label_orientation = 45\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
    "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
    "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
    "\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = dev_inp[::500]\n",
    "\n",
    "trans, states = model.translate_lines(inp)\n",
    "\n",
    "# select attention probs from model state (you may need to change this for your custom model)\n",
    "attention_probs = np.stack([state[-1] for state in states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    draw_attention(inp[i], trans[i], attention_probs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Finale (4+ points)\n",
    "\n",
    "We want you to find the best model for the task. Use everything you know.\n",
    "\n",
    "* different recurrent units: rnn/gru/lstm; deeper architectures\n",
    "* bidirectional encoder, different attention methods for decoder\n",
    "* word dropout, training schedules, anything you can imagine\n",
    "\n",
    "As usual, we want you to describe what you tried and what results you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`[your report/log here or anywhere you please]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
