__Lecture [slides](https://github.com/yandexdataschool/nlp_course/raw/master/resources/slides/nlp18_04_seq2seq_attention.pdf)__

Seminar [colab url](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/master/week04_seq2seq/practice.ipynb).

#### RNN Encoder-Decoder (Vanilla):
![vanilla_enc_dec](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/vanilla_enc_dec.gif)

#### Attention mechanism:
![attention_mechanism](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/attention_mechanism.gif)

### Materials
* Stanford lecture on seq2seq and MT - [video](https://www.youtube.com/watch?v=IxQtK2SjWWM)

### More on RNN
* Distill.pub post on attention and augmentations for RNN - [post](https://distill.pub/2016/augmented-rnns/)
* Seq2seq lecture - [video](https://www.youtube.com/watch?v=G5RY_SUJih4)
* [BLEU](http://www.aclweb.org/anthology/P02-1040.pdf) and [CIDEr](https://arxiv.org/pdf/1411.5726.pdf) articles.
* Image captioning
  * MSCOCO captioning [challenge](http://mscoco.org/dataset/#captions-challenge2015)
  * Captioning baseline [notebook](https://github.com/yandexdataschool/Practical_DL/tree/fall18/week07_seq2seq)
* Lecture on attention mechanisms - [video](https://www.youtube.com/watch?v=_XRBlhzb31U) (RUSSIAN)

### More on Transformer
* Illustrated transformer [post](https://jalammar.github.io/illustrated-transformer/)

### Practice
This time we're gonna use a shared `practice.ipynb` for both seminar and homework.
