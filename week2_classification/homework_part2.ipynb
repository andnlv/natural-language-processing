{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The grand quest: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either pure __tensorflow__ or __keras__. Feel free to adapt the seminar code for your needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Train_rev1.csv\", index_col=None).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubbish_columns = ['Id', 'LocationRaw', 'SalaryRaw', 'SourceName', 'SalaryNormalized']\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "import sklearn.preprocessing\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_general(data):\n",
    "    new_data = pd.DataFrame(index=data.index)\n",
    "    for column in text_columns + categorical_columns:\n",
    "        new_data[column] = data[column]\n",
    "    new_data = new_data.fillna('NaN')\n",
    "    return new_data, np.log1p(data['SalaryNormalized']).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessCategorical(TransformerMixin):\n",
    "    def __init__(self, min_company=40, min_city=20):\n",
    "        self.le_category = sklearn.preprocessing.LabelEncoder()\n",
    "        self.le_type = sklearn.preprocessing.LabelEncoder()\n",
    "        self.city_counter = Counter()\n",
    "        self.company_counter = Counter()\n",
    "        self.min_company = min_company\n",
    "        self.min_city = min_city\n",
    "        self.mean = {}\n",
    "        \n",
    "    def _fit_category(self, data):\n",
    "        self.le_category.fit(np.concatenate([data['Category'], ['other']]))\n",
    "        \n",
    "    def _fit_type(self, data):\n",
    "        self.le_type.fit(data['ContractTime'] + data['ContractType'])\n",
    "        \n",
    "    def _fit_city_company(self, data, what, minimum, target):\n",
    "        keys = {value if count >= minimum else 'other' for value, count in Counter(data[what]).most_common()}\n",
    "        tmp_data = pd.DataFrame(index=data.index)\n",
    "        tmp_data[what] = data[what].apply(lambda value: value if value in keys else 'other')\n",
    "        tmp_data['Salary'] = target\n",
    "        self.mean[what] = tmp_data.groupby(what)['Salary'].mean()\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        self._fit_category(data)\n",
    "        self._fit_city_company(data, 'LocationNormalized', self.min_city, target)\n",
    "        self._fit_city_company(data, 'Company', self.min_company, target)\n",
    "        self._fit_type(data)\n",
    "        return self\n",
    "    \n",
    "    def _transform_category(self, data, new_data):\n",
    "        new_data['Category'] = self.le_category.transform(data['Category'])\n",
    "    \n",
    "    def _transform_type(self, data, new_data):\n",
    "        new_data['Contract'] = self.le_type.transform(data['ContractTime'] + data['ContractType'])\n",
    "        \n",
    "        \n",
    "    def _transform_city_company(self, data, what, new_data):\n",
    "        new_data[what] = data[what].apply(lambda value: self.mean[what].get(value, self.mean[what]['other']))\n",
    "        \n",
    "    def transform(self, data):\n",
    "        new_data = pd.DataFrame(index=data.index)\n",
    "        self._transform_category(data, new_data)\n",
    "        self._transform_type(data, new_data)\n",
    "        self._transform_city_company(data, 'LocationNormalized', new_data)\n",
    "        self._transform_city_company(data, 'Company', new_data)\n",
    "        for column in data:\n",
    "            if column not in categorical_columns:\n",
    "                new_data[column] = data[column]\n",
    "        return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText(TransformerMixin):\n",
    "    def __init__(self, min_count = 10):\n",
    "        self.tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "        self.min_count = min_count\n",
    "        \n",
    "    def fit(self, data, target=None):\n",
    "        token_counts = Counter(' '.join(data['Title']).split())\n",
    "        token_counts += Counter(' '.join(data['FullDescription']).split())\n",
    "        self.tokens = [\"PAD\", \"UNK\"] + [token for token, count in token_counts.items() if count >= self.min_count]\n",
    "        self.token_to_id = {token: index for index, token in enumerate(self.tokens)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        new_data = pd.DataFrame(index=data.index)\n",
    "        for column in data:\n",
    "            new_data[column] = data[column].apply(\n",
    "                lambda text: list(map(lambda word: self.token_to_id.get(word, 0), \n",
    "                                      self.tokenizer.tokenize(str(text).lower()))\n",
    "                                 )\n",
    "            ) if column in text_columns else data[column]\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, target = preprocess_general(data)\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.3, random_state=325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PreprocessCategorical at 0x7fb94049fe10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_text = PreprocessText()\n",
    "prep_cat = PreprocessCategorical()\n",
    "prep_text.fit(data_train)\n",
    "prep_cat.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_train = prep_text.transform(prep_cat.transform(data_train))\n",
    "data_prep_test = prep_text.transform(prep_cat.transform(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_train.to_csv('data/data_prep_train.csv')\n",
    "data_prep_test.to_csv('data/data_prep_test.csv')\n",
    "target_train.to_csv('data/target_train.csv')\n",
    "target_test.to_csv('data/target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Contract</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>Company</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208221</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>10.206763</td>\n",
       "      <td>10.349338</td>\n",
       "      <td>[53046, 39461, 58514]</td>\n",
       "      <td>[53046, 39461, 40384, 47312, 30875, 39461, 585...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108637</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10.109263</td>\n",
       "      <td>10.725160</td>\n",
       "      <td>[2560, 43962, 23460, 29577]</td>\n",
       "      <td>[2560, 43962, 23460, 0, 16588, 11192, 23826, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86363</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10.350903</td>\n",
       "      <td>10.618861</td>\n",
       "      <td>[18976, 30453, 37004, 34174, 56318, 0]</td>\n",
       "      <td>[18976, 30453, 37004, 34174, 56318, 0, 12746, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21821</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10.534687</td>\n",
       "      <td>10.274894</td>\n",
       "      <td>[3221, 2321, 6533]</td>\n",
       "      <td>[3221, 6046, 46209, 62312, 34174, 21891, 4038,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156983</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10.315285</td>\n",
       "      <td>10.482319</td>\n",
       "      <td>[0, 51220]</td>\n",
       "      <td>[38991, 30453, 2091, 44206, 32096, 58837, 5815...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category  Contract  LocationNormalized    Company  \\\n",
       "208221        15         1           10.206763  10.349338   \n",
       "108637        11         3           10.109263  10.725160   \n",
       "86363         14         6           10.350903  10.618861   \n",
       "21821          8         6           10.534687  10.274894   \n",
       "156983         8         7           10.315285  10.482319   \n",
       "\n",
       "                                         Title  \\\n",
       "208221                   [53046, 39461, 58514]   \n",
       "108637             [2560, 43962, 23460, 29577]   \n",
       "86363   [18976, 30453, 37004, 34174, 56318, 0]   \n",
       "21821                       [3221, 2321, 6533]   \n",
       "156983                              [0, 51220]   \n",
       "\n",
       "                                          FullDescription  \n",
       "208221  [53046, 39461, 40384, 47312, 30875, 39461, 585...  \n",
       "108637  [2560, 43962, 23460, 0, 16588, 11192, 23826, 1...  \n",
       "86363   [18976, 30453, 37004, 34174, 56318, 0, 12746, ...  \n",
       "21821   [3221, 6046, 46209, 62312, 34174, 21891, 4038,...  \n",
       "156983  [38991, 30453, 2091, 44206, 32096, 58837, 5815...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>Category</th>\n",
       "      <th>Company</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Band 7 Paediatric Occupational Therapist North...</td>\n",
       "      <td>Pulse are urgently looking to recruit a Band 7...</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175325</th>\n",
       "      <td>Senior Mechanical Engineer  Water Operations</td>\n",
       "      <td>Senior Mechanical Engineer  Water Operations B...</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>Executive Recruitment Services</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53815</th>\n",
       "      <td>Sales and Marketing Manager</td>\n",
       "      <td>Sales and Marketing Manager Profile Yolk Recru...</td>\n",
       "      <td>Travel Jobs</td>\n",
       "      <td>Yolk Recruitment</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218504</th>\n",
       "      <td>C++ Developer FX Derivatives</td>\n",
       "      <td>Job Role: C++ Developer Location: London, City...</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>Client Server Ltd.</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199858</th>\n",
       "      <td>Retail Store Manager</td>\n",
       "      <td>Topps Tiles is the No.**** Tiling Specialist i...</td>\n",
       "      <td>Customer Services Jobs</td>\n",
       "      <td>TOPPS TILES</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30349</th>\n",
       "      <td>Shea Gas Joiners</td>\n",
       "      <td>must have valid tickets **** 6 weeks work This...</td>\n",
       "      <td>Trade &amp; Construction Jobs</td>\n",
       "      <td>RMF Construction Services Ltd.</td>\n",
       "      <td>Lymm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150701</th>\n",
       "      <td>Registration Services Advisor</td>\n",
       "      <td>This successful company based in Woking are se...</td>\n",
       "      <td>Admin Jobs</td>\n",
       "      <td>Faith Recruitment</td>\n",
       "      <td>Woking</td>\n",
       "      <td>part_time</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117366</th>\n",
       "      <td>Care Worker  St Ives and Surrounding villages</td>\n",
       "      <td>Carer ‘I love being able to help others, no ma...</td>\n",
       "      <td>Customer Services Jobs</td>\n",
       "      <td>Allied Healthcare</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164432</th>\n",
       "      <td>Principle Design Engineer</td>\n",
       "      <td>This World leading manufacturer of test machin...</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High Wycombe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161058</th>\n",
       "      <td>Application Sales Engineer  Capital Equipment</td>\n",
       "      <td>Application Sales Engineer  Capital Equipment ...</td>\n",
       "      <td>Sales Jobs</td>\n",
       "      <td>ATA</td>\n",
       "      <td>North East England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "377     Band 7 Paediatric Occupational Therapist North...   \n",
       "175325       Senior Mechanical Engineer  Water Operations   \n",
       "53815                         Sales and Marketing Manager   \n",
       "218504                       C++ Developer FX Derivatives   \n",
       "199858                               Retail Store Manager   \n",
       "30349                                    Shea Gas Joiners   \n",
       "150701                      Registration Services Advisor   \n",
       "117366      Care Worker  St Ives and Surrounding villages   \n",
       "164432                          Principle Design Engineer   \n",
       "161058      Application Sales Engineer  Capital Equipment   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "377     Pulse are urgently looking to recruit a Band 7...   \n",
       "175325  Senior Mechanical Engineer  Water Operations B...   \n",
       "53815   Sales and Marketing Manager Profile Yolk Recru...   \n",
       "218504  Job Role: C++ Developer Location: London, City...   \n",
       "199858  Topps Tiles is the No.**** Tiling Specialist i...   \n",
       "30349   must have valid tickets **** 6 weeks work This...   \n",
       "150701  This successful company based in Woking are se...   \n",
       "117366  Carer ‘I love being able to help others, no ma...   \n",
       "164432  This World leading manufacturer of test machin...   \n",
       "161058  Application Sales Engineer  Capital Equipment ...   \n",
       "\n",
       "                         Category                         Company  \\\n",
       "377     Healthcare & Nursing Jobs                             NaN   \n",
       "175325           Engineering Jobs  Executive Recruitment Services   \n",
       "53815                 Travel Jobs                Yolk Recruitment   \n",
       "218504                    IT Jobs              Client Server Ltd.   \n",
       "199858     Customer Services Jobs                     TOPPS TILES   \n",
       "30349   Trade & Construction Jobs  RMF Construction Services Ltd.   \n",
       "150701                 Admin Jobs               Faith Recruitment   \n",
       "117366     Customer Services Jobs               Allied Healthcare   \n",
       "164432                    IT Jobs                             NaN   \n",
       "161058                 Sales Jobs                             ATA   \n",
       "\n",
       "        LocationNormalized ContractType ContractTime  \n",
       "377                 London    full_time          NaN  \n",
       "175325             Bristol          NaN    permanent  \n",
       "53815                   UK          NaN    permanent  \n",
       "218504              London          NaN    permanent  \n",
       "199858              Bodmin          NaN    permanent  \n",
       "30349                 Lymm          NaN     contract  \n",
       "150701              Woking    part_time          NaN  \n",
       "117366                  UK          NaN    permanent  \n",
       "164432        High Wycombe          NaN    permanent  \n",
       "161058  North East England          NaN    permanent  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(data):\n",
    "    result = {}\n",
    "    for column in [\"Title\", \"FullDescription\"]:\n",
    "        max_len = max(map(len, data[column]))\n",
    "        result[column] = np.zeros([data.shape[0], max_len])\n",
    "        for index, line in enumerate(data[column]):\n",
    "            result[column][index][:len(line)] = line\n",
    "            \n",
    "    for col in [\"Category\", \"Contract\", \"LocationNormalized\", \"Company\"]:\n",
    "        result[col] = np.array(data[col].values).reshape(data.shape[0])\n",
    "    return result\n",
    "\n",
    "def iter_batches(data, target=None, batch_size=100, shuffle=True, cycle=False):\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            index = indices[start:start+batch_size]\n",
    "            if target is not None:\n",
    "                yield make_input(data.iloc[index]), target[index]\n",
    "            else:\n",
    "                yield make_input(data.iloc[index])\n",
    "                \n",
    "        if not cycle: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_model(hid_size=32, category_size=15, contract_size=6):\n",
    "    category = L.Input(shape=(1,), name=\"Category\")\n",
    "    contract = L.Input(shape=(1,), name=\"Contract\")\n",
    "    location = L.Input(shape=(1,), name=\"LocationNormalized\")\n",
    "    company = L.Input(shape=(1,), name=\"Company\")\n",
    "    emb_category = L.Reshape((category_size,))(L.Embedding(29, category_size)(category))\n",
    "    emb_contract = L.Reshape((contract_size,))(L.Embedding(9, contract_size)(contract))\n",
    "    hidden_0 = L.Concatenate()([emb_category, emb_contract, location, company])\n",
    "    hidden_1 = L.Dense(units=hid_size, activation='relu')(hidden_0)\n",
    "    hidden_2 = L.Dense(units=hid_size, activation='relu')(hidden_1)\n",
    "    output = L.Dense(units=1)(hidden_2)\n",
    "    model = keras.models.Model(inputs=[category, contract, location, company], outputs=output)\n",
    "    return model\n",
    "\n",
    "def create_textual_model(emb_size=128):\n",
    "    vocab_size=len(prep_text.tokens)\n",
    "    \n",
    "    title = L.Input(shape=(None,), name='Title')\n",
    "    descr = L.Input(shape=(None,), name='FullDescription')\n",
    "    emb_title = L.Embedding(vocab_size, emb_size)(title)\n",
    "    emb_descr = L.Embedding(vocab_size, emb_size)(descr)\n",
    "    conv_title = L.Conv1D(kernel_size=(2,), filters=32, activation='relu')(emb_title)\n",
    "    conv_descr = L.Conv1D(kernel_size=(4,), filters=64, activation='relu')(emb_descr)\n",
    "    pool_title = L.GlobalMaxPool1D()(conv_title)\n",
    "    pool_descr = L.GlobalMaxPool1D()(conv_descr)\n",
    "    print(title.shape)\n",
    "    recur_title_forward = L.LSTM(units=16, activation='relu')(emb_title)\n",
    "    recur_descr_forward = L.LSTM(units=32, activation='relu')(emb_descr)\n",
    "    recur_title_backward = L.LSTM(units=16, go_backwards=True, activation='relu')(emb_title)\n",
    "    recur_descr_backward = L.LSTM(units=32, go_backwards=True, activation='relu')(emb_descr)\n",
    "    hidden_0 = L.Concatenate()([\n",
    "        pool_title, pool_descr, \n",
    "        recur_title_forward, recur_descr_forward, \n",
    "        recur_title_backward, recur_descr_backward\n",
    "    ])\n",
    "    hidden_1 = L.Dense(emb_size, activation='relu')(hidden_0)\n",
    "    output = L.Dense(1)(hidden_1)\n",
    "    model = keras.models.Model(inputs=[title, descr], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_model(emb_size=10):\n",
    "    vocab_size=len(prep_text.tokens)\n",
    "    title = L.Input(shape=(None,), name='Title')\n",
    "    emb_title = L.Embedding(vocab_size, emb_size)(title)\n",
    "    conv_title = L.Conv1D(kernel_size=(2, ), filters=64, activation='relu')(emb_title)\n",
    "    pool_title = L.GlobalMaxPool1D()(conv_title)\n",
    "    output = L.Dense(1)(pool_title)\n",
    "    model = keras.models.Model(inputs=title, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_118 (Embedding)    (None, None, 10)          630960    \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, None, 64)          1344      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_17 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 632,369\n",
      "Trainable params: 632,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_testing_model()\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-384-bb2f6aede6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-379-3e1584552932>\u001b[0m in \u001b[0;36miter_batches\u001b[0;34m(data, target, batch_size, shuffle, cycle)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miter_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "next(iter_batches(data, train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/series.py:696: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 56ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 2s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 2s 15ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9336ede80>"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = 100\n",
    "model.fit_generator(\n",
    "    iter_batches(data_prep_train, target_train),\n",
    "    validation_data=iter_batches(data_prep_test, target_test),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=10,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Series' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-377-bb7de4049c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-375-ff16caac584b>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[0;34m(data, batch_size, shuffle, cycle, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "next(iterate_minibatches(data_prep_train, target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FullDescription (InputLayer)    (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_110 (Embedding)       (None, None, 128)    8076288     Title[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_111 (Embedding)       (None, None, 128)    8076288     FullDescription[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, None, 32)     8224        embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, None, 64)     32832       embedding_111[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 32)           0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 16)           9280        embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 32)           20608       embedding_111[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                  (None, 16)           9280        embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_49 (LSTM)                  (None, 32)           20608       embedding_111[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 192)          0           global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 lstm_46[0][0]                    \n",
      "                                                                 lstm_47[0][0]                    \n",
      "                                                                 lstm_48[0][0]                    \n",
      "                                                                 lstm_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 128)          24704       concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 1)            129         dense_70[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 16,278,241\n",
      "Trainable params: 16,278,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_textual_model()\n",
    "model.summary()\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "233077",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-397-add422ddb01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.fit(make_input(data_prep_train), \n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-396-65e70aac1bcd>\u001b[0m in \u001b[0;36mmake_input\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Contract\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LocationNormalized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Company\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 233077"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.fit(make_input(data_prep_train), \n",
    "          target_train, \n",
    "          validation_data=(make_input(data_prep_test), target_test), \n",
    "          epochs=3\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.920802 9.784760475158691\n",
      "\n",
      "10.574045 10.96959400177002\n",
      "\n",
      "10.624446 10.915106773376465\n",
      "\n",
      "10.527648 10.389025688171387\n",
      "\n",
      "10.514307 9.615805625915527\n",
      "\n",
      "10.631734 10.95955753326416\n",
      "\n",
      "10.673228 10.505094528198242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-31e8d877d403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, target in zip(data_prep_train.index, target_train):\n",
    "    print(model.predict(make_input(data_prep_train.loc[[i]]))[0][0], target)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time - our `L.GlobalMaxPool1D`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not want to use __`.get_keras_embedding()`__ method for word2vec\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback](https://keras.io/callbacks/#earlystopping).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
